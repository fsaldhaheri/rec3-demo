{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS4_fa2222.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsaldhaheri/rec3-demo/blob/main/PS4_fa2222.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIy_8z4fU3dv"
      },
      "source": [
        "---\n",
        "---\n",
        "Problem Set 4: Web Scraping\n",
        "\n",
        "Introduction to Computer Programming\n",
        "\n",
        "New York University, Abu Dhabi\n",
        "\n",
        "Out: 17th January 2022 || **Due: 20th January 2022 at 23:59**\n",
        "\n",
        "---\n",
        "---\n",
        "#Start Here\n",
        "## Learning Goals\n",
        "### General Goals\n",
        "- Learn the fundamental concepts of Web Scraping\n",
        "\n",
        "### Specific Goals\n",
        "- Learn how to identify websites that allow scraping\n",
        "- Learn the basics of BeautifulSoup\n",
        "- Learn to adjust the scraping code to more than one query\n",
        "\n",
        "## Collaboration Policy\n",
        "- You are expected to comply with the [University Policy on Academic Integrity and Plagiarism](https://www.nyu.edu/about/policies-guidelines-compliance/policies-and-guidelines/academic-integrity-for-students-at-nyu.html).\n",
        "- You are allowed to talk with / work with other students on homework assignments.\n",
        "- You can share ideas but not code, analyses or results; you must submit your own code and results. All submitted code will be compared against all code submitted this semester and online using MOSS. We will also critically analyze the similarities in the submitted reports, methodologies, and results, **but we will not police you**. We expect you all to be responsible enough to finish your work with full integrity.\n",
        "\n",
        "## Late Submission Policy\n",
        "You can submit the homework up to 2 late days. However, we will deduct 20% of your homework grade **for each late day you take**. We will not accept the homework after 2 late days.  Late submissions should be sent via email after the online submission is closed.\n",
        "\n",
        "## Distribution of Class Materials\n",
        "These problem sets and recitations are intellectual property of NYUAD, and we request the students to **not** distribute them or their solutions to other students who have not signed up for this class, and/or intend to sign up in the future. We also request you don't post these problem sets, and recitations online or on any public platforms.\n",
        "\n",
        "## Disclaimer\n",
        "The number of points do not necessarily signify/correlate to the difficulty level of the tasks.\n",
        "\n",
        "## Submission\n",
        "You will submit all your code as a Python Notebook, as well as the 2 CSV files obtained from Part II through [Brightspace](https://brightspace.nyu.edu/) as **P4_YOUR NETID.ipynb**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntO7OjkhUOvp"
      },
      "source": [
        "# General Instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzxk0RpUyTCC"
      },
      "source": [
        "This homework is worth 100 points. It has 3 parts. Below each part, we provide a set of concepts required to complete that part. All the parts need to be completed in a Jupyter (Colab) Notebook attached with this handout. We recommend that you read the complete handout before starting the homework.\n",
        "\n",
        "<font color=\"red\">**Important Note:** Please scrape the websites ethically and make sure you utilize the `sleep` function between requests. </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWE1SZaqQm31"
      },
      "source": [
        "# Part I: To Scrape or Not to Scrape (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p11YK7l3TXWm"
      },
      "source": [
        "A friend has a brilliant idea and wants us to write a program that will scrape product data from 3 different websites, and will recommend which one to buy based on price and reviews. However, we need to figure out which of the below websites actually allow scraping:\n",
        "\n",
        "- [Amazon.ae](https://www.amazon.ae/)\n",
        "- [Jumbo.ae](https://www.jumbo.ae/)\n",
        "- [Instock.ae](https://www.instock.ae/)\n",
        "\n",
        "Provide your answers, and, in a couple of sentences, describe how you have come to your answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_nPhLBFF3ssj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lt48_jiT4W_"
      },
      "source": [
        "\n",
        "Amazon.ae does not allow scraping for the English webpages with the exception of the wishlist creation page. A user needs to be a member in amazon.ae to access what' beyond these pages. However, amazon allows scraping the website in its Arabic version.\n",
        "\n",
        "Allow: /wishlist/universal\n",
        "Allow: /wishlist/vendor-button\n",
        "Allow: /wishlist/get-button\n",
        "Allow: /gp/wishlist/universal\n",
        "Allow: /gp/wishlist/vendor-button\n",
        "Allow: /gp/wishlist/ipad-install\n",
        "Allow: /-/ar/\n",
        "\n",
        "Jumbo.ae allows web scraping except for pages under /prodcuts/ and does not allow to access it's search engine. Moreover, it has a (Crawl-delay: 30) for user agents.\n",
        "\n",
        "Disallow: /products/\n",
        "\n",
        "Disallow: /search?q=*\n",
        "\n",
        "\n",
        "Finally, Instock.ae does not allow web scraping on its website.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7dlpv62CDum"
      },
      "source": [
        "### *Concepts required to complete this task:*\n",
        "\n",
        "- Concepts of Ethical Scraping\n",
        "\n",
        "Basic common sense applies to web scraping just like it would apply when interacting with other humans. Simple manners, such as asking for permission and thanking the host page, go a long way. Below are some basic rules to follow written by by Jami from [Empirical Data](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping).\n",
        "\n",
        "> \n",
        "1.   Just like knoking on the door of a house requesting permission to enter, a web scraper would ask for permission to access the website if not clearly stated.\n",
        "\n",
        "\n",
        "2.   Just like introducing yourself to people who don't know you, a webscraper would give some information about their identity by their User-Agent string.\n",
        "\n",
        "3.   Knowing the rules is a must in a civil society. Similarly, being aware of how terms and conditions,and robot.txt govern your behavior on a page reflects user prudence.\n",
        "\n",
        "4.   Use the data that you need and not more. Use it with consideration without causing any financial harm to the source that provided you with this information.\n",
        "\n",
        "5.   Finally in your research and final output. Thank and referece the source that provided you the information without excecting anything in return."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DojUO82FXqEv"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +5 points for correct answer and pointing the place you found the information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrIBbFv7UHyj"
      },
      "source": [
        "# Part II: Hindi Geet Mala (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBCZYVp67t13"
      },
      "source": [
        "[Hindi Geet Mala](https://www.hindigeetmala.net/) is a website containing information about Indian movies, songs, singers, etc. For this part, you will scrape information about all the movies in **alphabetical order**. Additionally, you will scrape information about songs. \n",
        "\n",
        "More precisely, you will submit 2 CSV files:\n",
        "\n",
        "* movies.csv: Title, Year, Number of Songs, Film Director, Film Producer, Film cast, Lyricist, Music Director, Singer, External links, Watch Full Movie\n",
        "\n",
        "* songs.csv: Artists, Title, Rating, Number of Votes, Movie Title\n",
        "\n",
        "*Note: you are NOT allowed to manually write the list of letters or use a list generator for the alphabet. The list should be retrieved from the page.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "import time"
      ],
      "metadata": {
        "id": "4j7t2W7bSSYg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to scrape movies' number or ratings on hindigeetmala.net\n",
        "\n",
        "header ={'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'}\n",
        "\n",
        "\n",
        "# scrape the homepage for years in which movies were produced\n",
        "url = \"https://www.hindigeetmala.net\"\n",
        "\n",
        "# Sending request and getting content\n",
        "req = requests.get(url)\n",
        "soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "years = soup.find_all('a', attrs={'class':'head1'})\n",
        "\n",
        "# creating an empty dataframe that will store movie titles,\n",
        "# year of prodcution, and number of ratings\n",
        "movies = {'Title': [0], 'Year': [0], 'Number of Songs': [0],'Film Director': [0],'Film Producer': [0],\n",
        "          'Film cast': [0],'Lyricist': [0],'Music Director': [0],'Singer': [0],'External links': [0],'Watch Full Movie': [0],}\n",
        "movies_df = pd.DataFrame(data = movies)\n",
        "movies_df = movies_df.drop(movies_df.index[0]).reset_index(drop=True)\n",
        "\n",
        "songs = {'Title': [0], 'Year': [0], 'Number of Songs': [0],'Film Director': [0],'Film Producer': [0],\n",
        "          'Film cast': [0],'Lyricist': [0],'Music Director': [0],'Singer': [0],'External links': [0],'Watch Full Movie': [0],}\n",
        "songs_df = pd.DataFrame(data = movies)\n",
        "songs_df = songs.drop(songs.index[0]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "# setting empty URL to preserve years\n",
        "url_movies = set()\n",
        "\n",
        "# loop and extract years from homepage\n",
        "for i in years:\n",
        "    if (i.get('href')[0:6] == '/movie') and (i.get('href')[-8:-5].isnumeric()):\n",
        "        # year will be added as a string to homepage URL\n",
        "        year = i.get('href').replace(\"/movie/\",\"\").replace(\".php\",\"\").replace(\"_\",\" \")\n",
        "        # creating a pandas series to add to the dataframe df\n",
        "        year_df = pd.Series({\"Year\":year})\n",
        "        # URL of each year\n",
        "        for dummy in range(1,2):\n",
        "            url_movies = url+str(i.get('href'))+'?page='+str(dummy)\n",
        "            # print(year, i.get('href'), url_movies)\n",
        "\n",
        "            # Sending request to get content and filter out movies' URL\n",
        "            time.sleep(1)\n",
        "            req = requests.get(url_movies)\n",
        "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "            table = soup.find('table', attrs={'class':'b1 w760 alcen'})\n",
        "            movies = table.find_all('a', attrs={'href':True})\n",
        "\n",
        "            # empty string to avoid storing redundant listings of the same movie\n",
        "            ''' how can we stop pages from repeating '''\n",
        "            redundant_movie = ''\n",
        "\n",
        "            # extracting movie names & URLs, and adding them to a pandas series\n",
        "            for movie in movies:\n",
        "                time.sleep(1)\n",
        "                if (movie.get('href')[0:6] == '/movie') and (movie.get('href')[-3:]=='htm'):\n",
        "                    if (movie.get('href') != redundant_movie):\n",
        "                        redundant_movie = movie.get('href')\n",
        "                        movie_name = movie.get('href').replace(\"/movie/\",\"\").replace(\".htm\",\"\").replace(\"_\",\" \")\n",
        "                        name_df = pd.Series({\"Movie\":movie_name})\n",
        "                        #print(year_df, name_df, movie_name, movie.get('href'),movie['href'])\n",
        "\n",
        "                        # on the movies page, we count the number of songs by adding up the rating count\n",
        "                        url_final = \"https://www.hindigeetmala.net\"+str(movie['href'])\n",
        "                        \n",
        "                        # print(url_final)\n",
        "                        time.sleep(1)\n",
        "                        req = requests.get(url_final)\n",
        "                        soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "                        table = soup.find('td', attrs={'class':'w760 vatop alcen'})\n",
        "                        songs = table.find_all('span', attrs={'itemprop':'ratingCount'})\n",
        "\n",
        "                        counter=0\n",
        "                        for song in songs:\n",
        "                            counter += 1\n",
        "                        ratings = counter\n",
        "                        ratings_df = pd.Series({\"Ratings\":ratings})\n",
        "\n",
        "                        # append each row to the dataframe\n",
        "                        new_row = {'Movie': movie_name, 'Year': year, 'Count': ratings}\n",
        "                        df = df.append(new_row, ignore_index=True)\n",
        "                        \n",
        "# drop the first row\n",
        "df = df.drop(df.index[0]).reset_index(drop=True)\n",
        "print(df)\n",
        "\n",
        "#the end"
      ],
      "metadata": {
        "id": "_ipjChnCr_tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYKOgyUxr0HU"
      },
      "source": [
        "############# SOLUTION ###############\n",
        "##### Solution start ########\n",
        "\n",
        "\n",
        "# needed to go through the pages in alphabatecial order\n",
        "# one more page is 0-9 which we add after the loop\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# loop through the pages in alphabetical order\n",
        "for i in range(len(alpha)):\n",
        "    url_alpha = \"https://www.hindigeetmala.net/movie/\"+str(alpha[i])+\".php\"\n",
        "    response = requests.get(url_alpha)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    \n",
        "    # extract last 4 characters to get max number of pages per letter\n",
        "    last_page = soup.title.string[-5:]\n",
        "    # remove letters and spaces and leave a number\n",
        "    last_page = int(last_page.translate({ord(i): None for i in 'of '}))\n",
        "    \n",
        "    #looping through each letter and all of the pages under it.\n",
        "    for i in range(1, last_page+1):\n",
        "        # taking above url and adding numbers\n",
        "        url_number = url_alpha + '?page=' + str(i)\n",
        "        \n",
        "        # request and organize content of webpage \n",
        "        response = requests.get(url_number)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # extract the names of all images on page\n",
        "        names = soup.find_all('img', alt=True)\n",
        "        \n",
        "        # remove names of non-movie images.\n",
        "        namelist = []\n",
        "        for name in names:\n",
        "            if (name['alt'] != 'new') & (name['alt'] != 'Movie') & (name['alt'] != 'next page') & (name['alt'][-3:] != 'jpg'):\n",
        "                namelist.append(name['alt'])\n",
        "                print(namelist)\n",
        "\n",
        "# later\n",
        "# we are testing out namelist on a single page\n",
        "# this is part of the for loop above but i am seperating it to avoid the time\n",
        "# needed to run the loop\n",
        "# \n",
        "url_number = \"https://www.hindigeetmala.net/movie/a.php?page=3\"\n",
        "response = requests.get(url_number)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "names = soup.find_all('img', alt=True)\n",
        "\n",
        "namelist = []\n",
        "for name in names:\n",
        "    #print(name['alt'])\n",
        "    if ((name['alt'] == 'new') | \n",
        "        (name['alt'] == 'Movie') | \n",
        "        (name['alt'] == 'next page') | \n",
        "        (name['alt'][-3:] == 'jpg')):\n",
        "        pass\n",
        "    elif (name['alt'][-1] == ')'):\n",
        "        if (name['alt'][-6] == '('):\n",
        "            pass\n",
        "    else:\n",
        "        namelist.append(name['alt'])\n",
        "        print(namelist)\n",
        "\n",
        "# extract the years of each movie to average but this number later\n",
        "# next is to make a url with the name of the movies at the end attached\n",
        "# loop through each movie page and gather all songs and count them.\n",
        "# print a datafram with the average of each year and the year next to it\n",
        "############# SOLUTION END ###############\n",
        "\n",
        "'''\n",
        "\n",
        "url = \"https://www.hindigeetmala.net\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "alpha = soup.find_all('a', attrs={'class':'head1'})\n",
        "\n",
        "for a in alpha:\n",
        "\n",
        "    # movies urls and therefore information starts here\n",
        "    if 'movie' in a['href']:\n",
        "        # condition to avoid any string with a number\n",
        "        # we add 0-9 manually\n",
        "        if (not any(i.isdigit() for i in a['href'])):\n",
        "            print('movie string: ', a['href'])\n",
        "            \n",
        "            url_alpha = \"https://www.hindigeetmala.net\"+str(a['href'])\n",
        "            response = requests.get(url_alpha)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            \n",
        "            # extract last 4 characters to get max number of pages per letter\n",
        "            last_page = soup.title.string[-5:]\n",
        "            # remove letters and spaces and leave a number\n",
        "            last_page = int(last_page.translate({ord(i): None for i in 'of '}))\n",
        "            \n",
        "            #looping through each letter and all of the pages under it.\n",
        "            for i in range(1, last_page+1):\n",
        "                # taking above url and adding numbers\n",
        "                url_number = url_alpha + '?page=' + str(i)\n",
        "                \n",
        "                # request and organize content of webpage \n",
        "                response = requests.get(url_number)\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # extract the names of all images on page\n",
        "                names = soup.find_all('img', alt=True)\n",
        "                \n",
        "                # remove names of non-movie images.\n",
        "                namelist = []\n",
        "                for name in names:\n",
        "                    if (name['alt'] != 'new') & (name['alt'] != 'Movie') & (name['alt'] != 'next page') & (name['alt'][-3:] != 'jpg'):\n",
        "                        namelist.append(name['alt'])\n",
        "                        print(namelist)\n",
        "\n",
        "\n",
        "    # songs urls and therefore information starts here\n",
        "    elif 'index' not in a['href']:\n",
        "        print('singer string: ', a['href'])\n",
        "\n",
        "        '''\n",
        "        movies_df = pd.Series({})\n",
        "\n",
        "        if c.string != 'Category':\n",
        "            txt = c.string\n",
        "            cat_title = txt.split('/movies')[0]\n",
        "            freq = txt.split(' (')[1][:-1]\n",
        "            df2 = pd.Series({\"Category\":cat_title,\"Frequency\":freq})\n",
        "            df = df.append(df2, ignore_index=True)\n",
        "\n",
        "        df = pd.DataFrame(columns=[\"Category\",\"Frequency\"])\n",
        "        '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1: Finding URL alphabets for movies and songs "
      ],
      "metadata": {
        "id": "oxUhh9O_0nHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url = \"https://www.hindigeetmala.net\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "alpha = soup.find_all('a', attrs={'class':'head1'})\n",
        "\n",
        "for a in alpha:\n",
        "    # movies urls and therefore information starts here\n",
        "    if 'movie' in a['href']:\n",
        "        # condition to avoid any string with a number\n",
        "        # we add 0-9 manually\n",
        "        if (not any(i.isdigit() for i in a['href'])):\n",
        "            print('movie string:', a['href'])\n",
        "    # songs urls and therefore information starts here\n",
        "    else:\n",
        "        if 'index' not in a['href']:\n",
        "            print('singer string: ', a['href'])"
      ],
      "metadata": {
        "id": "0ni7uiCpl60X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found all letters. We move to numbers"
      ],
      "metadata": {
        "id": "7tP7FFpr1jJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_alpha = \"https://www.hindigeetmala.net/movie/a.php\"\n",
        "response = requests.get(url_alpha)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# extract last 4 characters to get max number of pages per letter\n",
        "last_page = soup.title.string[-5:]\n",
        "# remove letters and spaces and leave a number\n",
        "last_page = int(last_page.translate({ord(i): None for i in 'of '}))\n",
        "      "
      ],
      "metadata": {
        "id": "K-DBLvZ316_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can loop through all pages under a letter"
      ],
      "metadata": {
        "id": "ZE4zZ4C62Xj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this code loops through numbers of each letter \n",
        "for i in range(1, last_page+1):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "RTndNuY7l7Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 2: obtaining URL movie names"
      ],
      "metadata": {
        "id": "xm_SZ3EA1Bwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "egU4byT0pPq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The snippets above and below are together but not to call the website everytime."
      ],
      "metadata": {
        "id": "8PIepwBe28RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name in names:\n",
        "    if ((name['alt'] != 'new') & (name['alt'] != 'Movie') & (name['alt'] != 'next page') & (name['alt'][-3:] != 'jpg')):\n",
        "        print(name['alt'])"
      ],
      "metadata": {
        "id": "LIJtAtxq24sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have the names. I need to append them to a series \n",
        "\n",
        "1.   remove year\n",
        "2.   add underscore\n",
        "3.   create URL"
      ],
      "metadata": {
        "id": "mn9nrV7C1QKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  remove year\n",
        "\n",
        "#   add underscore\n",
        "\n",
        "#   create movie page URL\n"
      ],
      "metadata": {
        "id": "QHTaAjivnvkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Extract info from movie page"
      ],
      "metadata": {
        "id": "J-6HHeZM0kQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parsed = json.loads(response.text)\n",
        "print(json.dumps(parsed, indent=4, sort_keys=True)) #this code prettifies the content for you for readability by indenting it"
      ],
      "metadata": {
        "id": "qZbuXmAFS-7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sAXny6fLTAG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape the whole page and view soup"
      ],
      "metadata": {
        "id": "Wf_aE_GTfqvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<title>\n",
        "   A - Alphabetically List of Hindi Films - Starting from A - Page 1 of 50\n",
        "  </title>\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "n3P7Lcp6gVpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_number = \"https://www.hindigeetmala.net/movie/a.php?page=3\"\n",
        "response = requests.get(url_number)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "EaEpPfifvrxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can either use find_all and then find\n",
        "table = soup.find_all('table', attrs={'class':'b1 w760 alcen'})\n",
        "names = table[0].find('img', attrs={'alt':True})"
      ],
      "metadata": {
        "id": "2-EY7EKBvyM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or we find all and then loop through each item\n",
        "names = table.find_all('img', attrs={'alt':True})\n",
        "for item in names:\n",
        "    item['alt']"
      ],
      "metadata": {
        "id": "jhml6sKrv4dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a second way where we start with find then find all\n",
        "table = soup.find('table', attrs={'class':'b1 w760 alcen'})\n",
        "names = table.find_all('img', attrs={'alt':True})\n",
        "print(names)"
      ],
      "metadata": {
        "id": "juOej2szv62x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q9x9FmCKv-BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking above url and adding numbers\n",
        "url = \"https://www.hindigeetmala.net/movie/aakhree_raasta.htm\"\n",
        "    \n",
        "# request and organize content of webpage \n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "#print(soup.prettify())  # We call prettify for easier viewing\n",
        "\n",
        "# movie title\n",
        "movie_title = soup.title.string.split(' :')[0]\n",
        "# movie year\n",
        "movie_year = soup.title.string[-5:-1]\n",
        "\n",
        "# print(soup.title)\n",
        "actors = soup.find_all(\"span\", itemprop=\"name\")\n",
        "#actors = soup.find_all(\"td\", itemprop=\"actor\")\n",
        "ratings = soup.find_all(\"span\", itemprop=\"ratingValue\")\n",
        "\n",
        "ratingcount = soup.find_all(\"span\", itemprop=\"ratingCount\")\n",
        "\n",
        "counter=0\n",
        "for count in ratingcount:\n",
        "    print(count.string)\n",
        "    counter+=1\n",
        "\n",
        "# number of songs\n",
        "number_of_songs = counter\n",
        "\n",
        "for rate in ratings:\n",
        "    print(rate.string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXZBG2cvnv3Y",
        "outputId": "6a15d7ba-b2ba-4cbb-dfe3-443afc0047ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "6\n",
            "3\n",
            "12\n",
            "4\n",
            "1\n",
            "6\n",
            "3.41\n",
            "4.67\n",
            "4.67\n",
            "3.50\n",
            "4.75\n",
            "1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to extract actors"
      ],
      "metadata": {
        "id": "abv-ufK-jww3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for actor in actors:\n",
        "    # while (actor.string != name):\n",
        "    # Each of the elements in this list is a span object\n",
        "    print(actor.string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdYBqqJLjvGP",
        "outputId": "682b4703-3c3b-4480-8fab-f56d7c2f4f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Gori Ka Sajan Sajan Ki Gori \n",
            "Aakhree Raasta\n",
            " Gori Kaa Saajan \n",
            "Aakhree Raasta\n",
            " Hey You Pahale Padhaai Phir Pyaar Hogaa \n",
            "Aakhree Raasta\n",
            " Tune Mera Dudh Piya Hai Tu Bilkul Mere Jaisa Hai \n",
            "Aakhree Raasta\n",
            " Tera Dudh Aisa Hai \n",
            "Aakhree Raasta\n",
            " Dance Music (Aakhree Raasta) \n",
            "Aakhree Raasta\n",
            "Aakhree Raasta\n",
            "Amitabh Bachchan\n",
            "Jaya Prada\n",
            "Sridevi\n",
            "Anupam Kher\n",
            "Sadashiv Amrapurkar\n",
            "Om Shiv Puri\n",
            "Dalip Tahil\n",
            "Bharat Kapoor\n",
            "Viju Khote\n",
            "Gurbachan\n",
            "Jagdarshan\n",
            "Umesh Sharma\n",
            "Vijay Kumar\n",
            "Anand\n",
            "Ashok Kumar\n",
            "Kiran\n",
            "Hamid\n",
            "Dilip\n",
            "Laxmikant  Pyarelal\n",
            "K Bhagyaraj\n",
            "Purnachandra Rao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start extracting information we select"
      ],
      "metadata": {
        "id": "2AL49h_bf21D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the names of all images on page\n",
        "names = soup.find_all('meta', alt=True)\n",
        "print(names[2])"
      ],
      "metadata": {
        "id": "I4HzwwxPf0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "endpoint = \"http://www.thecocktaildb.com/api/json/v1/1/search.php?s=margarita\"\n",
        "response = requests.get(endpoint)\n",
        "response.text\n",
        "# To make this more readable, let's use the json library\n",
        "import json\n",
        "\n",
        "parsed = json.loads(response.text)\n",
        "print(json.dumps(parsed, indent=4, sort_keys=True)) #this code prettifies the content for you for readability by indenting it"
      ],
      "metadata": {
        "id": "7JXvQytnnwSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON16kQeAAYgm"
      },
      "source": [
        "### *Concepts required to complete this task:*\n",
        "\n",
        "- Navigating through HTML code using functions\n",
        "- DataFrame Creation and Writing to a file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv-gC50LgGO2"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +30 points for correct output (15 points for each dataframe)\n",
        "- +15 points for concise, logical code and strategic scraping\n",
        "- +3 points for ethical and mindful scraping\n",
        "- +2 points for comments and variable names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC4EGarx7vh9"
      },
      "source": [
        "# Part III: Yahoo Finance (45 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZELErl5z7yqA"
      },
      "source": [
        "Write a function called `stock_decision()` that will take as an input the list of company abbreviations (e.g. the URL for Facebook is https://finance.yahoo.com/quote/FB/history/, so this company's abbreviation is \"FB\"), and will scrape the stock market close/last data for the given companies from Yahoo Finance, compute the weekly average for the past 3 weeks and check whether or not it has been increasing. If the weekly average has been steadily increasing, recommend to buy, otherwise recommend to sell.\n",
        "\n",
        "The output should be two lines: \n",
        "\n",
        "```\n",
        "\"Sell: \"\n",
        "\n",
        "\"Buy: \"\n",
        "```\n",
        "\n",
        "When you implement the function, call it with these companies: \n",
        "- Apple\n",
        "- Microsoft\n",
        "- Amazon\n",
        "- Tesla\n",
        "- Facebook \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*Hint: For grouping the data per week, look up `resample` function from Pandas. Another function that will come in handy is `diff` from Pandas.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShRFrKvGsPfX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e1c9d88-d82f-470a-ab70-3858cbffdc15"
      },
      "source": [
        "############# SOLUTION ###############\n",
        "\n",
        "header = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'}\n",
        "headers = header\n",
        "url = 'https://finance.yahoo.com/quote/ASPL.L/history'\n",
        "\n",
        "# time.sleep(1)\n",
        "req = requests.get(url)\n",
        "print(req.status_code)\n",
        "print(req.text)\n",
        "soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "table = soup.find('table', attrs={'class':'W(100%) M(0)'})\n",
        "price = table.find_all('td', attrs={'class':True})\n",
        "\n",
        "print(price)\n",
        "\n",
        "############# SOLUTION END ###############"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "404\n",
            "<!DOCTYPE html>\n",
            "  <html lang=\"en-us\"><head>\n",
            "  <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\n",
            "      <meta charset=\"utf-8\">\n",
            "      <title>Yahoo</title>\n",
            "      <meta name=\"viewport\" content=\"width=device-width,initial-scale=1,minimal-ui\">\n",
            "      <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\n",
            "      <style>\n",
            "  html {\n",
            "      height: 100%;\n",
            "  }\n",
            "  body {\n",
            "      background: #fafafc url(https://s.yimg.com/nn/img/sad-panda-201402200631.png) 50% 50%;\n",
            "      background-size: cover;\n",
            "      height: 100%;\n",
            "      text-align: center;\n",
            "      font: 300 18px \"helvetica neue\", helvetica, verdana, tahoma, arial, sans-serif;\n",
            "  }\n",
            "  table {\n",
            "      height: 100%;\n",
            "      width: 100%;\n",
            "      table-layout: fixed;\n",
            "      border-collapse: collapse;\n",
            "      border-spacing: 0;\n",
            "      border: none;\n",
            "  }\n",
            "  h1 {\n",
            "      font-size: 42px;\n",
            "      font-weight: 400;\n",
            "      color: #400090;\n",
            "  }\n",
            "  p {\n",
            "      color: #1A1A1A;\n",
            "  }\n",
            "  #message-1 {\n",
            "      font-weight: bold;\n",
            "      margin: 0;\n",
            "  }\n",
            "  #message-2 {\n",
            "      display: inline-block;\n",
            "      *display: inline;\n",
            "      zoom: 1;\n",
            "      max-width: 17em;\n",
            "      _width: 17em;\n",
            "  }\n",
            "      </style>\n",
            "  <script>\n",
            "    document.write('<img src=\"//geo.yahoo.com/b?s=1197757129&t='+new Date().getTime()+'&src=aws&err_url='+encodeURIComponent(document.URL)+'&err=%<pssc>&test='+encodeURIComponent('%<{Bucket}cqh[:200]>')+'\" width=\"0px\" height=\"0px\"/>');var beacon = new Image();beacon.src=\"//bcn.fp.yahoo.com/p?s=1197757129&t=\"+new Date().getTime()+\"&src=aws&err_url=\"+encodeURIComponent(document.URL)+\"&err=%<pssc>&test=\"+encodeURIComponent('%<{Bucket}cqh[:200]>');\n",
            "  </script>\n",
            "  </head>\n",
            "  <body>\n",
            "  <!-- status code : 404 -->\n",
            "  <!-- Not Found on Server -->\n",
            "  <table>\n",
            "  <tbody><tr>\n",
            "      <td>\n",
            "      <img src=\"https://s.yimg.com/rz/p/yahoo_frontpage_en-US_s_f_p_205x58_frontpage.png\" alt=\"Yahoo Logo\">\n",
            "      <h1 style=\"margin-top:20px;\">Will be right back...</h1>\n",
            "      <p id=\"message-1\">Thank you for your patience.</p>\n",
            "      <p id=\"message-2\">Our engineers are working quickly to resolve the issue.</p>\n",
            "      </td>\n",
            "  </tr>\n",
            "  </tbody></table>\n",
            "  </body></html>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a5d2d226ea42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'W(100%) M(0)'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'td'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgbfC_N7BbFV"
      },
      "source": [
        "### *Concepts required to complete this task:*\n",
        "\n",
        "- Navigating through HTML code using functions\n",
        "- DataFrame Creation and Manipulations\n",
        "- Applying functions to a DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqnMGCkcOd-O"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +25 points for scraping the correct information\n",
        "- +10 points for concise and logical code\n",
        "- +5 points for correctly and efficiently calculating the averages, percent change\n",
        "- +3 points for ethical and mindful scraping\n",
        "- +2 points for comments and logical variable names"
      ]
    }
  ]
}